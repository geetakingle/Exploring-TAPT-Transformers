{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import (\n",
    "    get_AGNews_datasets,\n",
    "    train, test, accuracy,\n",
    "    dynamic_masking,\n",
    "    RobertaMLM_with_classifier\n",
    ")\n",
    "%env WANDB_PROJECT=TAPT_roberta\n",
    "%env WANDB_LOG_MODEL='end'\n",
    "\n",
    "# Import relevant models, tokenizers, and related libs\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Statics\n",
    "DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n",
    "SEED = 69\n",
    "SEEDED_GEN = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "# Confirm device type, should say CUDA if you have a GPU\n",
    "print(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See utils.py for all helper classes and model definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Explore\n",
    "\n",
    "What are the lengths of sentences? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "path = join(\"data\",f\"train.csv\")\n",
    "df = pd.read_csv(path)\n",
    "df['encoded'] = df['Description'].apply(lambda x: tokenizer(x)['input_ids'])\n",
    "df['maxlength'] = df['encoded'].apply(lambda x: len(x))\n",
    "display(df['maxlength'].describe())\n",
    "\n",
    "from os.path import join\n",
    "path = join(\"data\",f\"test.csv\")\n",
    "df = pd.read_csv(path)\n",
    "df['encoded'] = df['Description'].apply(lambda x: tokenizer(x)['input_ids'])\n",
    "df['maxlength'] = df['encoded'].apply(lambda x: len(x))\n",
    "display(df['maxlength'].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "#### 1) Task Adaptive Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, LineByLineTextDataset\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "import datasets\n",
    "\n",
    "hyperparams_TAPT = {\n",
    "    \"EPOCHS\" : 20,\n",
    "    \"MASK_PROB\" : 0.1,\n",
    "    'TRAINING_BATCH_SIZE' : 32,\n",
    "    \"MAX_LEN\" : 77\n",
    "}\n",
    "\n",
    "model_type = \"distilroberta-base\"\n",
    "checkpoint = \"huggingface/TAPT_businessTAPT_1682554710\"\n",
    "robertaMLM_model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "# dataset = datasets.load_dataset(\"ag_news\")\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='data/traincommaed.csv',\n",
    "    block_size=hyperparams_TAPT['MAX_LEN'],\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=hyperparams_TAPT['MASK_PROB'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='checkpoints/TAPT_Roberta_DAPT_TAPT',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=hyperparams_TAPT['EPOCHS'],\n",
    "    per_device_train_batch_size=hyperparams_TAPT['TRAINING_BATCH_SIZE'],\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=10,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=robertaMLM_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "# Save model\n",
    "curr_time = int(time.time())\n",
    "model_folder = f'huggingface/DAPT_TAPT_{curr_time}'\n",
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Domain Adaptive Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, LineByLineTextDataset\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "import datasets\n",
    "\n",
    "hyperparams_TAPT = {\n",
    "    \"EPOCHS\" : 5,\n",
    "    \"MASK_PROB\" : 0.1,\n",
    "    'TRAINING_BATCH_SIZE' : 32,\n",
    "    \"MAX_LEN\" : 77\n",
    "}\n",
    "\n",
    "model_type = \"distilroberta-base\"\n",
    "robertaMLM_model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "# dataset = datasets.load_dataset(\"ag_news\")\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='data/businessnews.csv',\n",
    "    block_size=hyperparams_TAPT['MAX_LEN'],\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=hyperparams_TAPT['MASK_PROB'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='checkpoints/BusinessDAPT_Roberta',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=hyperparams_TAPT['EPOCHS'],\n",
    "    per_device_train_batch_size=hyperparams_TAPT['TRAINING_BATCH_SIZE'],\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=5,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=robertaMLM_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "# Save model\n",
    "curr_time = int(time.time())\n",
    "model_folder = f'huggingface/TAPT_businessTAPT_{curr_time}'\n",
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fine Tuning for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common configs\n",
    "tokenizer_type = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n",
    "\n",
    "hyperparams = {\n",
    "    \"TRAIN_PCT\" : 0.9,\n",
    "    \"TRAIN_BATCH_SIZE\" : 200,\n",
    "    \"VALID_BATCH_SIZE\" : 200,\n",
    "    \"TEST_BATCH_SIZE\" : 200,\n",
    "    \"MAX_LEN\" : 77,\n",
    "    \"EPOCHS\" : 25,\n",
    "    \"LR\" : 0.005,\n",
    "    \"L2_REG\" : 0.000000,\n",
    "    \"ADAM_BETAS\" : (0.87, 0.98),\n",
    "    \"ADAM_EPS\" : 1e-6,\n",
    "    \"FC_HIDDEN\" : 768,\n",
    "    \"FC_DROPOUT\" : 0.05,\n",
    "    \"SCH_ENDFACTOR\" : 0.1,\n",
    "    \"RUN_SUFFIX\" : \"_9\"\n",
    "}\n",
    "\n",
    "project_name = \"TAPT_roberta\"\n",
    "\n",
    "### --- LOAD DATA --------------\n",
    "train_dataset, valid_dataset, test_dataset = get_AGNews_datasets(\n",
    "    tokenizer,\n",
    "    DEVICE,\n",
    "    max_length=hyperparams['MAX_LEN'],\n",
    "    train_pct=hyperparams['TRAIN_PCT'],\n",
    "    generator=SEEDED_GEN\n",
    ")\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=hyperparams['TRAIN_BATCH_SIZE'], shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=hyperparams['VALID_BATCH_SIZE'], shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=hyperparams['TEST_BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "# Test data loading\n",
    "display(next(iter(train_dataloader))['encoding'].shape)\n",
    "display(train_dataset[5].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Pre-TAPT Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"distilroberta-base\"\n",
    "MLM_layers = AutoModelForMaskedLM.from_pretrained(model_type).roberta\n",
    "\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    job_type='pre_TAPT_finetune',\n",
    "    name=f\"20EPOCH-TAPT-FC_{hyperparams['RUN_SUFFIX']}\",\n",
    "    config=hyperparams\n",
    ")\n",
    "\n",
    "### --- TRAIN MODEL ----------------------\n",
    "# Define Model and freeze pre-trained layers\n",
    "model = RobertaMLM_with_classifier(MLM_layers, fc_hidden=hyperparams['FC_HIDDEN'], fc_dropout=hyperparams['FC_DROPOUT'])\n",
    "model.to(DEVICE)\n",
    "for param in model.mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.classifier.parameters(),\n",
    "    betas=hyperparams['ADAM_BETAS'],\n",
    "    eps=hyperparams['ADAM_EPS'],\n",
    "    lr=hyperparams['LR'],\n",
    "    weight_decay=hyperparams['L2_REG']\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=hyperparams['SCH_ENDFACTOR'], total_iters=hyperparams['EPOCHS'])\n",
    "\n",
    "# Perform Training and Testing\n",
    "for epoch in range(hyperparams['EPOCHS']):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Num. Training Batches = {len(train_dataloader)}\")\n",
    "    print(f\"Num. Validation Batches = {len(valid_dataloader)}\")\n",
    "    print(f\"Num. Test Batches = {len(test_dataloader)}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    train(epoch+1, model, train_dataloader, valid_dataloader, optimizer, criterion, wandb)\n",
    "    scheduler.step()\n",
    "test(model, test_dataloader, criterion, wandb)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save model\n",
    "curr_pretrain_time = int(time.time())\n",
    "PATH = f\"models/{model_type}_base_finetuned_{curr_pretrain_time}.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) Post-TAPT Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = f\"huggingface/DAPT_TAPT_1682693602\"\n",
    "\n",
    "MLM_layers = AutoModelForMaskedLM.from_pretrained(model_type).roberta\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n",
    "\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    job_type='post_TAPT_finetune',\n",
    "    name=f\"20EPOCH-DAPT_TAPT_FC_{hyperparams['RUN_SUFFIX']}\",\n",
    "    config=hyperparams\n",
    ")\n",
    "\n",
    "### --- TRAIN MODEL ----------------------\n",
    "# Define Model and freeze pre-trained layers\n",
    "final_model = RobertaMLM_with_classifier(MLM_layers, fc_hidden=hyperparams['FC_HIDDEN'], fc_dropout=hyperparams['FC_DROPOUT'])\n",
    "final_model.to(DEVICE)\n",
    "for param in final_model.mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=final_model.classifier.parameters(),\n",
    "    betas=hyperparams['ADAM_BETAS'],\n",
    "    eps=hyperparams['ADAM_EPS'],\n",
    "    lr=hyperparams['LR'],\n",
    "    weight_decay=hyperparams['L2_REG']\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=hyperparams['SCH_ENDFACTOR'], total_iters=hyperparams['EPOCHS'])\n",
    "\n",
    "# Perform Training and Testing\n",
    "for epoch in range(hyperparams['EPOCHS']):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Num. Training Batches = {len(train_dataloader)}\")\n",
    "    print(f\"Num. Validation Batches = {len(valid_dataloader)}\")\n",
    "    print(f\"Num. Test Batches = {len(test_dataloader)}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    train(epoch+1, final_model, train_dataloader, valid_dataloader, optimizer, criterion, wandb)\n",
    "    scheduler.step()\n",
    "test(final_model, test_dataloader, criterion, wandb)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save model\n",
    "curr_time = int(time.time())\n",
    "PATH = f\"models/DAPT_TAPT_1682693602_finetuned_{curr_time}.pt\"\n",
    "torch.save(final_model.state_dict(), PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrix\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model from disk\n",
    "model_type = \"distilroberta-base\"\n",
    "# This path would be the related saved model - either TAPT, DAPT, DAPT+TAPT\n",
    "PATH = f\"models/TAPT_1682353322_finetuned_1682474239.pt\"\n",
    "MLM_layers = AutoModelForMaskedLM.from_pretrained(model_type).roberta\n",
    "lazarus_model = RobertaMLM_with_classifier(MLM_layers, fc_hidden=hyperparams['FC_HIDDEN'], fc_dropout=hyperparams['FC_DROPOUT'])\n",
    "lazarus_model.load_state_dict(torch.load(PATH))\n",
    "lazarus_model.to(DEVICE)\n",
    "lazarus_model.eval()\n",
    "for param in lazarus_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform classification and analysis\n",
    "tot_batches = len(test_dataloader)\n",
    "total_acc = 0.\n",
    "predicted = []\n",
    "target = []\n",
    "for data in tqdm(iter(test_dataloader)):\n",
    "    encodings = data['encoding'].squeeze(dim=1)\n",
    "    masks = data['mask'].squeeze(dim=1)\n",
    "    targets = data['label']\n",
    "    \n",
    "    output = lazarus_model(encodings, masks)\n",
    "    pred = torch.argmax(output, dim=-1)\n",
    "    acc = accuracy(output, targets)\n",
    "    total_acc += acc\n",
    "\n",
    "    predicted.append(pred)\n",
    "    target.append(targets)\n",
    "\n",
    "predicted = torch.cat(predicted)\n",
    "target = torch.cat(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(predicted.cpu(), target.cpu()))\n",
    "\n",
    "# '0': World\n",
    "# '1': Sports\n",
    "# '2': Business\n",
    "# '3': Sci/Tech\n",
    "\n",
    "# True label on ith (dim 0), predicted on jth (dim 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_confusion_matrix(predicted.cpu(), target.cpu()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
